{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code and Equations for a 3-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can ignore these functions. I used them to generate\n",
    "# all of the matrices. I left them in for those who are\n",
    "# into that sort of thing.\n",
    "\n",
    "\n",
    "def print_bvector(template, row):\n",
    "    row_minus_one = str(row - 1) if isinstance(row, int) else f\"{row}-1\"\n",
    "    print(\"\\\\begin{bmatrix}\")\n",
    "    print(f\"{template.replace('row', '1')} \\\\\\\\\")\n",
    "    print(f\"{template.replace('row', '2')} \\\\\\\\\")\n",
    "    print(\"\\\\vdots \\\\\\\\\")\n",
    "    print(f\"{template.replace('row', row_minus_one)} \\\\\\\\\")\n",
    "    print(f\"{template.replace('row', str(row))} \\\\\\\\\")\n",
    "    print(\"\\\\end{bmatrix}\")\n",
    "\n",
    "\n",
    "def print_bvector_row(template, col):\n",
    "    col_minus_one = str(col - 1) if isinstance(col, int) else f\"{col}-1\"\n",
    "    print(\"\\\\begin{bmatrix}\")\n",
    "    print(f\"{template.replace('col', '1')} &\")\n",
    "    print(f\"{template.replace('col', '2')} &\")\n",
    "    print(\"\\\\cdots &\")\n",
    "    print(f\"{template.replace('col', col_minus_one)} &\")\n",
    "    print(f\"{template.replace('col', str(col))}\")\n",
    "    print(\"\\\\end{bmatrix}\")\n",
    "\n",
    "\n",
    "def print_bmatrix(template, row, col, compress=False):\n",
    "\n",
    "    row_minus_one = str(row - 1) if isinstance(row, int) else f\"{row}-1\"\n",
    "    col_minus_one = str(col - 1) if isinstance(col, int) else f\"{col}-1\"\n",
    "\n",
    "    print(\"\\\\begin{bmatrix}\")\n",
    "    # Row 1\n",
    "    c1 = template.replace(\"col\", \"1\").replace(\"row\", \"1\")\n",
    "    c2 = template.replace(\"col\", \"2\").replace(\"row\", \"1\")\n",
    "    c3 = template.replace(\"col\", col_minus_one).replace(\"row\", \"1\")\n",
    "    c4 = template.replace(\"col\", str(col)).replace(\"row\", \"1\")\n",
    "    if compress:\n",
    "        print(f\"{c1} & {c2} & \\\\cdots & {c4}\\\\\\\\\")\n",
    "    else:\n",
    "        print(f\"{c1} & {c2} & \\\\cdots & {c3} & {c4}\\\\\\\\\")\n",
    "    c1 = template.replace(\"col\", \"1\").replace(\"row\", \"2\")\n",
    "    c2 = template.replace(\"col\", \"2\").replace(\"row\", \"2\")\n",
    "    c3 = template.replace(\"col\", col_minus_one).replace(\"row\", \"2\")\n",
    "    c4 = template.replace(\"col\", str(col)).replace(\"row\", \"2\")\n",
    "    if compress:\n",
    "        print(f\"{c1} & {c2} & \\\\cdots & {c4}\\\\\\\\\")\n",
    "        print(\"\\\\vdots & \\\\vdots & \\ddots & \\\\vdots \\\\\\\\\")\n",
    "    else:\n",
    "        print(f\"{c1} & {c2} & \\\\cdots & {c3} & {c4}\\\\\\\\\")\n",
    "        print(\"\\\\vdots & \\\\vdots & \\ddots & \\\\vdots & \\\\vdots \\\\\\\\\")\n",
    "    c1 = template.replace(\"col\", \"1\").replace(\"row\", row_minus_one)\n",
    "    c2 = template.replace(\"col\", \"2\").replace(\"row\", row_minus_one)\n",
    "    c3 = template.replace(\"col\", col_minus_one).replace(\"row\", row_minus_one)\n",
    "    c4 = template.replace(\"col\", str(col)).replace(\"row\", row_minus_one)\n",
    "    if not compress:\n",
    "        print(f\"{c1} & {c2} & \\\\cdots & {c3} & {c4}\\\\\\\\\")\n",
    "    c1 = template.replace(\"col\", \"1\").replace(\"row\", str(row))\n",
    "    c2 = template.replace(\"col\", \"2\").replace(\"row\", str(row))\n",
    "    c3 = template.replace(\"col\", col_minus_one).replace(\"row\", str(row))\n",
    "    c4 = template.replace(\"col\", str(col)).replace(\"row\", str(row))\n",
    "    if compress:\n",
    "        print(f\"{c1} & {c2} & \\\\cdots & {c4}\\\\\\\\\")\n",
    "    else:\n",
    "        print(f\"{c1} & {c2} & \\\\cdots & {c3} & {c4}\\\\\\\\\")\n",
    "    print(\"\\\\end{bmatrix}\")\n",
    "\n",
    "\n",
    "# print_bvector(\"x^{(i)}_{row}\", \"n_x\")\n",
    "# print_bvector(\"image^{(i)}_{pixel_{row}}\", \"n_x\")\n",
    "# print_bmatrix(\"x^{(col)}_{row}\", \"n_x\", \"m\")\n",
    "# print_bmatrix(\"y^{(col)}_{row}\", \"n_y\", \"m\")\n",
    "# print_bmatrix(\"x^{(col)}_{row}\", 784, 60000)\n",
    "# print_bmatrix(\"y^{(col)}_{row}\", 10, 60000)\n",
    "# print_bvector_row(\"y^{(col)}\", 60000)\n",
    "# print_bmatrix(\"w^{[l]}_{row,col}\", \"n_l\", \"n_{l-1}\")\n",
    "# print_bvector(\"b^{[l]}_{row}\", \"n_l\")\n",
    "\n",
    "# Computing Z\n",
    "# print_bmatrix(\"w^{[l]}_{row,col}\", \"n_l\", \"n_{l-1}\", True)\n",
    "# print_bmatrix(\"A^{(col)[l-1]}_{row}\", \"n_{l-1}\", \"m\", True)\n",
    "# print_bmatrix(\"w^{[l]}_{row} \\cdot a^{(col)[l-1]} + b^{[l]}_{row}\", \"n_l\", \"m\")\n",
    "\n",
    "# And A\n",
    "# print_bmatrix(\"g(w^{[l]}_{row} \\cdot a^{(col)[l-1]} + b^{[l]}_{row})\", \"n_l\", \"m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{X, Y\\}\n",
    "$$\n",
    "\n",
    "Where $D$ is the dataset and it comprises input features $X$ and output targets $Y$.\n",
    "\n",
    "The input features $X$ is a matrix containing all features of all input examples. **Let's use MNIST as our working example.** We represent a single image $x^{(i)}$ as a *column* vector:\n",
    "\n",
    "$$\n",
    "x^{(i)} =\n",
    "\\begin{bmatrix}\n",
    "x^{(i)}_{1} \\\\\n",
    "x^{(i)}_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x^{(i)}_{n_x-1} \\\\\n",
    "x^{(i)}_{n_x} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "image^{(i)}_{pixel_{1}} \\\\\n",
    "image^{(i)}_{pixel_{2}} \\\\\n",
    "\\vdots \\\\\n",
    "image^{(i)}_{pixel_{n_x-1}} \\\\\n",
    "image^{(i)}_{pixel_{n_x}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $n_x$ is the number of input features for a single example. For MNIST, $n_x = 28\\cdot28 = 784$ (and for our neural network, we'll say $n_0=n_x$).\n",
    "\n",
    "Each column in $X$ is a single input instance (called an example or sample), and when you stack all $m$ examples side-by-side, you end up with $X$.\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_{1} & x^{(2)}_{1} & \\cdots & x^{(m-1)}_{1} & x^{(m)}_{1}\\\\\n",
    "x^{(1)}_{2} & x^{(2)}_{2} & \\cdots & x^{(m-1)}_{2} & x^{(m)}_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x^{(1)}_{n_x-1} & x^{(2)}_{n_x-1} & \\cdots & x^{(m-1)}_{n_x-1} & x^{(m)}_{n_x-1}\\\\\n",
    "x^{(1)}_{n_x} & x^{(2)}_{n_x} & \\cdots & x^{(m-1)}_{n_x} & x^{(m)}_{n_x}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We say that $x^{(i)} \\in \\mathcal{R}^{n_x}$ (each input example is $n_x$ real values) and $X \\in \\mathcal{R}^{n_x \\times m}$ (the entire input is a $(n_x, m)$ matrix).\n",
    "\n",
    "The tagets (aka labels) are in the matrix $Y$. For MNIST, $Y$ will be a $(1, m)$ matrix, but in general, $Y$ is $(n_y, m)$. **Note, I've been a bit inconsistent about the shape of $Y$ on diagrams and in code. From here out, I will follow this notation.**\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)}_{1} & y^{(2)}_{1} & \\cdots & y^{(m-1)}_{1} & y^{(m)}_{1}\\\\\n",
    "y^{(1)}_{2} & y^{(2)}_{2} & \\cdots & y^{(m-1)}_{2} & y^{(m)}_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "y^{(1)}_{n_y-1} & y^{(2)}_{n_y-1} & \\cdots & y^{(m-1)}_{n_y-1} & y^{(m)}_{n_y-1}\\\\\n",
    "y^{(1)}_{n_y} & y^{(2)}_{n_y} & \\cdots & y^{(m-1)}_{n_y} & y^{(m)}_{n_y}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We say that $y^{(i)} \\in \\mathcal{R}^{n_y}$ (each target is $n_y$ real values) and $Y \\in \\mathcal{R}^{n_y \\times m}$ (the entire input is a $(n_y, m)$ matrix). For classification tasks, each $y^{(i)}_j$ (each value for each target) is usually $\\in \\{0, 1\\}$.\n",
    "\n",
    "For MNIST, we have:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_{1} & x^{(2)}_{1} & \\cdots & x^{(59999)}_{1} & x^{(60000)}_{1}\\\\\n",
    "x^{(1)}_{2} & x^{(2)}_{2} & \\cdots & x^{(59999)}_{2} & x^{(60000)}_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x^{(1)}_{783} & x^{(2)}_{783} & \\cdots & x^{(59999)}_{783} & x^{(60000)}_{783}\\\\\n",
    "x^{(1)}_{784} & x^{(2)}_{784} & \\cdots & x^{(59999)}_{784} & x^{(60000)}_{784}\\\\\n",
    "\\end{bmatrix}\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)}_{1} & y^{(2)}_{1} & \\cdots & y^{(59999)}_{1} & y^{(60000)}_{1}\\\\\n",
    "y^{(1)}_{2} & y^{(2)}_{2} & \\cdots & y^{(59999)}_{2} & y^{(60000)}_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "y^{(1)}_{9} & y^{(2)}_{9} & \\cdots & y^{(59999)}_{9} & y^{(60000)}_{9}\\\\\n",
    "y^{(1)}_{10} & y^{(2)}_{10} & \\cdots & y^{(59999)}_{10} & y^{(60000)}_{10}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For MNIST when we are working with just two classes:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_{1} & x^{(2)}_{1} & \\cdots & x^{(59999)}_{1} & x^{(60000)}_{1}\\\\\n",
    "x^{(1)}_{2} & x^{(2)}_{2} & \\cdots & x^{(59999)}_{2} & x^{(60000)}_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x^{(1)}_{783} & x^{(2)}_{783} & \\cdots & x^{(59999)}_{783} & x^{(60000)}_{783}\\\\\n",
    "x^{(1)}_{784} & x^{(2)}_{784} & \\cdots & x^{(59999)}_{784} & x^{(60000)}_{784}\\\\\n",
    "\\end{bmatrix}\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)} &\n",
    "y^{(2)} &\n",
    "\\cdots &\n",
    "y^{(59999)} &\n",
    "y^{(60000)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Let's create some random data of the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([100, 10000])\n",
      "Shape of Y: torch.Size([4, 10000])\n",
      "All outputs for the first five examples\n",
      " tensor([[1, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "nx = 100\n",
    "ny = 4\n",
    "\n",
    "X = torch.randn(nx, m)\n",
    "Y = torch.randint(2, size=(ny, m))\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "print(\"All outputs for the first five examples\\n\", Y[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating parameters for a single layer\n",
    "\n",
    "Each layer of the neural network has $n_l$ neruons. Each of these neurons are connected to each of the $n_{l-1}$ neruons from the previous layer. These connections are referred to as \"weights,\" since they dictate how each value from the previous layer is weighed in this layers calculations.\n",
    "\n",
    "$$\n",
    "W^{[l]} = \n",
    "\\begin{bmatrix}\n",
    "w^{[l]}_{1,1} & w^{[l]}_{1,2} & \\cdots & w^{[l]}_{1,n_{l-1}-1} & w^{[l]}_{1,n_{l-1}}\\\\\n",
    "w^{[l]}_{2,1} & w^{[l]}_{2,2} & \\cdots & w^{[l]}_{2,n_{l-1}-1} & w^{[l]}_{2,n_{l-1}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "w^{[l]}_{n_l-1,1} & w^{[l]}_{n_l-1,2} & \\cdots & w^{[l]}_{n_l-1,n_{l-1}-1} & w^{[l]}_{n_l-1,n_{l-1}}\\\\\n",
    "w^{[l]}_{n_l,1} & w^{[l]}_{n_l,2} & \\cdots & w^{[l]}_{n_l,n_{l-1}-1} & w^{[l]}_{n_l,n_{l-1}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each weight matrix is $(n_l, n_{l-1})$ in shape. The first row includes all weights connected to the top neuron, the second row includes all weights connected to the second neuron, and so on.\n",
    "\n",
    "Along with weights, we have a bias term for each neuron.\n",
    "\n",
    "$$\n",
    "b^{[l]} =\n",
    "\\begin{bmatrix}\n",
    "b^{[l]}_{1} \\\\\n",
    "b^{[l]}_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b^{[l]}_{n_l-1} \\\\\n",
    "b^{[l]}_{n_l} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Bias is a column vector of size $(n_l, 1)$.\n",
    "\n",
    "Note that we do not have the parenthesis superscript as we do for $X$ and $Y$; that is because the same network weights are applied to all inputs.\n",
    "\n",
    "## Let's randomly initialize the parameters (weights and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n0: 100, n1: 12, n2: 4\n",
      "  W1 (12, 100) : torch.Size([12, 100])\n",
      "  b1 (12, 1) : torch.Size([12, 1])\n",
      "  W2 (4, 12) : torch.Size([4, 12])\n",
      "  b2 (4, 1) : torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters(\n",
    "    n0: int, n1: int, n2: int, scale: float\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Initialize parameters for a 3-layer neural network.\n",
    "\n",
    "    Args:\n",
    "        n0 (int): Number of input features (aka nx)\n",
    "        n1 (int): Number of neurons in layer 1\n",
    "        n2 (int): Number of output neurons\n",
    "        scale (float): Scaling factor for parameters\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: weights and biases for 2 layers\n",
    "    \"\"\"\n",
    "    W1 = torch.randn(n1, n0) * scale\n",
    "    b1 = torch.zeros(n1, 1)\n",
    "    W2 = torch.randn(n2, n1) * scale\n",
    "    b2 = torch.zeros(n2, 1)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "n0 = nx\n",
    "n1 = torch.randint(40, (1, 1)).item()\n",
    "n2 = ny\n",
    "parameter_scale = torch.rand(1, 1).item()\n",
    "\n",
    "W1, b1, W2, b2 = initialize_parameters(n0, n1, n2, parameter_scale)\n",
    "\n",
    "print(f\"n0: {n0}, n1: {n1}, n2: {n2}\")\n",
    "print(\"  W1\", (n1, n0), \":\", W1.shape)\n",
    "print(\"  b1\", (n1, 1), \":\", b1.shape)\n",
    "print(\"  W2\", (n2, n1), \":\", W2.shape)\n",
    "print(\"  b2\", (n2, 1), \":\", b2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "The next step is to compute the output of the neural network. The output can go by a few names:\n",
    "\n",
    "- prediction (or pred)\n",
    "- $A^{[L]}$ (output/activation of the final layer)\n",
    "- $\\hat Y$\n",
    "\n",
    "Above, $L$, refers to the index of the final layer. For our neural network, we refer to $X$ as $A^{[0]}$ to denote that it is the values at layer zero in the network.\n",
    "\n",
    "Here are the two equations we need to implement for each layer:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]} &= W^{[l]} A^{[l-1]} + b^{[l]}\\\\\n",
    "A^{[l]} &= g(Z^{[l]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $g(\\cdot)$ is any activation function. Note that these two equations compute the linear and non-linear parts of all neurons in layer $l$.\n",
    "\n",
    "Let's look at these in more detail:\n",
    "\n",
    "$$\n",
    "Z^{[l]} = \n",
    "\\begin{bmatrix}\n",
    "w^{[l]}_{1,1} & w^{[l]}_{1,2} & \\cdots & w^{[l]}_{1,n_{l-1}}\\\\\n",
    "w^{[l]}_{2,1} & w^{[l]}_{2,2} & \\cdots & w^{[l]}_{2,n_{l-1}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{[l]}_{n_l,1} & w^{[l]}_{n_l,2} & \\cdots & w^{[l]}_{n_l,n_{l-1}}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a^{(1)[l-1]}_{1} & a^{(2)[l-1]}_{1} & \\cdots & a^{(m)[l-1]}_{1}\\\\\n",
    "a^{(1)[l-1]}_{2} & a^{(2)[l-1]}_{2} & \\cdots & a^{(m)[l-1]}_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(1)[l-1]}_{n_{l-1}} & a^{(2)[l-1]}_{n_{l-1}} & \\cdots & a^{(m)[l-1]}_{n_{l-1}}\\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b^{[l]}_{1} \\\\\n",
    "b^{[l]}_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b^{[l]}_{n_l} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here are the shapes: $(n_l, m) = (n_l, n_{l-1}) \\cdot (n_{l-1}, m) + (n_l, 1)$\n",
    "\n",
    "The bias vector is [broadcast](https://pytorch.org/docs/stable/notes/broadcasting.html) to a shape of $(n_l, m)$ so that it can be added to the result of the matrix multiplication.\n",
    "\n",
    "Here is $Z^{[l]}$ after the multiplication and addition:\n",
    "\n",
    "$$\n",
    "Z^{[l]} = \n",
    "\\begin{bmatrix}\n",
    "w^{[l]}_{1} \\cdot a^{(1)[l-1]} + b^{[l]}_{1} & w^{[l]}_{1} \\cdot a^{(2)[l-1]} + b^{[l]}_{1} & \\cdots & w^{[l]}_{1} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{1} & w^{[l]}_{1} \\cdot a^{(m)[l-1]} + b^{[l]}_{1}\\\\\n",
    "w^{[l]}_{2} \\cdot a^{(1)[l-1]} + b^{[l]}_{2} & w^{[l]}_{2} \\cdot a^{(2)[l-1]} + b^{[l]}_{2} & \\cdots & w^{[l]}_{2} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{2} & w^{[l]}_{2} \\cdot a^{(m)[l-1]} + b^{[l]}_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "w^{[l]}_{n_l-1} \\cdot a^{(1)[l-1]} + b^{[l]}_{n_l-1} & w^{[l]}_{n_l-1} \\cdot a^{(2)[l-1]} + b^{[l]}_{n_l-1} & \\cdots & w^{[l]}_{n_l-1} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{n_l-1} & w^{[l]}_{n_l-1} \\cdot a^{(m)[l-1]} + b^{[l]}_{n_l-1}\\\\\n",
    "w^{[l]}_{n_l} \\cdot a^{(1)[l-1]} + b^{[l]}_{n_l} & w^{[l]}_{n_l} \\cdot a^{(2)[l-1]} + b^{[l]}_{n_l} & \\cdots & w^{[l]}_{n_l} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{n_l} & w^{[l]}_{n_l} \\cdot a^{(m)[l-1]} + b^{[l]}_{n_l}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the equation above, don't let it go unnoticed that each cell (for example, $w^{[l]}_{1} \\cdot a^{(1)[l-1]} + b^{[l]}_{1}$) includes a dot product of $n_{l-1}$ values.\n",
    "\n",
    "All that's left is to apply the activation function to each value.\n",
    "\n",
    "$$\n",
    "A^{[l]} = \n",
    "\\begin{bmatrix}\n",
    "g(w^{[l]}_{1} \\cdot a^{(1)[l-1]} + b^{[l]}_{1}) & g(w^{[l]}_{1} \\cdot a^{(2)[l-1]} + b^{[l]}_{1}) & \\cdots & g(w^{[l]}_{1} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{1}) & g(w^{[l]}_{1} \\cdot a^{(m)[l-1]} + b^{[l]}_{1})\\\\\n",
    "g(w^{[l]}_{2} \\cdot a^{(1)[l-1]} + b^{[l]}_{2}) & g(w^{[l]}_{2} \\cdot a^{(2)[l-1]} + b^{[l]}_{2}) & \\cdots & g(w^{[l]}_{2} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{2}) & g(w^{[l]}_{2} \\cdot a^{(m)[l-1]} + b^{[l]}_{2})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "g(w^{[l]}_{n_l-1} \\cdot a^{(1)[l-1]} + b^{[l]}_{n_l-1}) & g(w^{[l]}_{n_l-1} \\cdot a^{(2)[l-1]} + b^{[l]}_{n_l-1}) & \\cdots & g(w^{[l]}_{n_l-1} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{n_l-1}) & g(w^{[l]}_{n_l-1} \\cdot a^{(m)[l-1]} + b^{[l]}_{n_l-1})\\\\\n",
    "g(w^{[l]}_{n_l} \\cdot a^{(1)[l-1]} + b^{[l]}_{n_l}) & g(w^{[l]}_{n_l} \\cdot a^{(2)[l-1]} + b^{[l]}_{n_l}) & \\cdots & g(w^{[l]}_{n_l} \\cdot a^{(m-1)[l-1]} + b^{[l]}_{n_l}) & g(w^{[l]}_{n_l} \\cdot a^{(m)[l-1]} + b^{[l]}_{n_l})\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 10000, n1: 12, n2: 4\n",
      "  A1 (12, 10000) : torch.Size([12, 10000])\n",
      "  A2 (4, 10000) : torch.Size([4, 10000]) torch.Size([4, 10000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1187, 0.8149, 0.0401, 0.6765, 0.6688],\n",
       "        [0.4339, 0.2414, 0.1625, 0.8435, 0.9407],\n",
       "        [0.8535, 0.4525, 0.7716, 0.5258, 0.8212],\n",
       "        [0.0522, 0.0170, 0.2146, 0.3959, 0.1363]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_propagation(\n",
    "    A0: Tensor, W1: Tensor, b1: Tensor, W2: Tensor, b2: Tensor\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Compute the output of a 3-layer neural network.\n",
    "\n",
    "    Args:\n",
    "        A0 (Tensor): (n0, m) input matrix (aka X)\n",
    "        W1 (Tensor): (n1, n0) weight matrix\n",
    "        b1 (Tensor): (n1, 1) bias matrix)\n",
    "        W2 (Tensor): (n2, n1) weight matrix)\n",
    "        b2 (Tensor): (n2, 1) bias matrix\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: outputs for layers 1 (n1, m) and 2 (n2, m)\n",
    "    \"\"\"\n",
    "    Z1 = W1 @ A0 + b1\n",
    "    A1 = torch.sigmoid(Z1)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = torch.sigmoid(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "\n",
    "A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "print(f\"m: {m}, n1: {n1}, n2: {n2}\")\n",
    "print(\"  A1\", (n1, m), \":\", A1.shape)\n",
    "print(\"  A2\", (n2, m), \":\", A2.shape, Y.shape)\n",
    "A2[: min(n2, 5), : min(m, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking above, we can see the following:\n",
    "\n",
    "- The sizes of $A^{[2]}$ and $Y$ are equal.\n",
    "- Values in $A^{[2]} \\in [0, 1]$ (range from $0$ to $1$) whereas $Y \\in \\{0, 1\\}$ (the set of $0$ and $1$).\n",
    "\n",
    "So, let's add a function that rounds these values to the nearest guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2\n",
      " tensor([[0.1187, 0.8149, 0.0401, 0.6765, 0.6688],\n",
      "        [0.4339, 0.2414, 0.1625, 0.8435, 0.9407],\n",
      "        [0.8535, 0.4525, 0.7716, 0.5258, 0.8212],\n",
      "        [0.0522, 0.0170, 0.2146, 0.3959, 0.1363]])\n",
      "Y\n",
      " tensor([[1, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "Predictions\n",
      " tensor([[0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def get_predictions_sigmoid(\n",
    "    A0: Tensor, W1: Tensor, b1: Tensor, W2: Tensor, b2: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"Convert the output of a sigmoid to zeros and ones.\n",
    "\n",
    "    Args:\n",
    "        A0 (Tensor): (n0, m) input matrix (aka X)\n",
    "        W1 (Tensor): (n1, n0) weight matrix\n",
    "        b1 (Tensor): (n1, 1) bias matrix)\n",
    "        W2 (Tensor): (n2, n1) weight matrix)\n",
    "        b2 (Tensor): (n2, 1) bias matrix\n",
    "\n",
    "    Returns:\n",
    "        Tensor: binary predictions of a 3-layer neural network\n",
    "    \"\"\"\n",
    "    _, A2 = forward_propagation(A0, W1, b1, W2, b2)\n",
    "    return A2.round()\n",
    "\n",
    "Yhat = get_predictions_sigmoid(X, W1, b1, W2, b2)\n",
    "\n",
    "print(\"A2\\n\", A2[:min(n2,5), :min(m, 5)])\n",
    "print(\"Y\\n\", Y[:min(n2,5), :min(m, 5)])\n",
    "print(\"Predictions\\n\", Yhat[:min(n2,5), :min(m, 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute an output, but the output is currently random. We probably have about a 50% accuracy. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4965), tensor(0.4965))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (Y - Yhat) # Get the difference\n",
    "absolute_error = error.abs() # Ignore the sign\n",
    "mean_error = absolute_error.mean() # Get the average\n",
    "accuracy = 1 - mean_error # This works since the output is either 0 or 1\n",
    "\n",
    "accuracy, 1 - (Y - Yhat).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need learn the parameters\n",
    "\n",
    "To make the error zero, we need some way to make $\\hat Y$ converge toward $Y$. (Note, we cannot always make the error exaclty zero, but we can often get close.)\n",
    "\n",
    "We cannot change $Y$ (it wouldn't make sense to relabel a cat as a dog), but we can change $\\hat Y$ by adjusting the parameters $W^{[l]}$ and $b^{[l]}$.\n",
    "\n",
    "But what is the best way to minimize them? In deep learning we use gradient descent. That is, we compute the gradient of an objective function with respect to the parameters we can change. This tells us *how* we should change the parameters (make them bigger or smaller).\n",
    "\n",
    "As we've discussed in class, we can use any of a number of objective functions. But let's focus on binary cross entropy loss so that we have a specific example:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat Y, Y) = -( Y \\log(\\hat Y) + (1 - Y) \\log(1 - \\hat Y))\n",
    "$$\n",
    "\n",
    "Now we just need to compute some derivatives. Namely (for the case of a 3-layer network), $\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}$, $\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}}$, $\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}$, and $\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}$.\n",
    "\n",
    "Starting with $W^{[2]}$ and applying the chain rule, we end up with this:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \n",
    "\\color{blue}{\\frac{\\partial \\mathcal{L}}{\\partial \\hat Y}} \\cdot\n",
    "\\color{green}{\\frac{\\partial \\hat Y}{\\partial Z^{[2]}}} \\cdot\n",
    "\\color{magenta}{\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}}\n",
    "$$\n",
    "\n",
    "We can compute each of these separately.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{blue}{\\frac{\\partial \\mathcal{L}}{\\partial \\hat Y}}\n",
    "&= -\\frac{\\partial}{\\partial \\hat Y}( Y \\log(\\hat Y) + (1 - Y) \\log(1 - \\hat Y))\\\\\n",
    "&= -(\\frac{Y}{\\hat Y} + \\frac{1-Y}{1-\\hat Y}\\cdot \\frac{\\partial 1 - \\hat Y}{\\partial \\hat Y})\\\\\n",
    "&= -\\frac{Y}{\\hat Y} + \\frac{1-Y}{1-\\hat Y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now let's compute the second component.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{green}{\\frac{\\partial \\hat Y}{\\partial Z^{[2]}}}\n",
    "&= \\frac{\\partial}{\\partial Z^{[2]}} g(Z^{[2]})\\\\\n",
    "&= g'(Z^{[2]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, $g'(\\cdot)$ will depend on the activation function. For the sigmoid function $\\sigma(\\cdot)$ it is $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$. Now the final component:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{magenta}{\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}}\n",
    "&= \\frac{\\partial}{\\partial W^{[2]}} W^{[2]} A^{[1]} + b^{[2]}\\\\\n",
    "&= A^{[1]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Putting this all together we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \n",
    "\\color{blue}{-\\frac{Y}{\\hat Y} + \\frac{1-Y}{1-\\hat Y}} \\cdot\n",
    "\\color{green}{g'(Z^{[2]})} \\cdot\n",
    "\\color{magenta}{A^{[1]}}\n",
    "$$\n",
    "\n",
    "For this example, let's assume we are using the sigmoid function. Simpligying and rearranging this equation so that the dimension match, we take into account that we should use the average over all $m$ examples, and the correct terms are computed, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}\n",
    "= \\frac{1}{m} (\\hat Y - Y) A^{[1]T}\n",
    "$$\n",
    "\n",
    "Next up, we need to perform a similar procedure for $\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}}$. You might notice that this term also includes $\\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}}$, which we've already computed as $(\\hat Y - Y)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}} \n",
    "&= dZ^{[2]} \\cdot \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}}\\\\\n",
    "&= (\\hat Y - Y) \\cdot \\frac{\\partial}{\\partial b^{[2]}} W^{[2]} A^{[1]} + b^{[2]}\\\\\n",
    "&= (\\hat Y - Y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But again, we want to take the average across all $m$ examples:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}} \n",
    "= \\frac{1}{m} \\sum_{i=1}^m (\\hat Y - Y)\n",
    "$$\n",
    "\n",
    "Note: averaging over all $m$ examples is done implicitly by the matrix multiplications when computing the derivative of $W^{[2]}$.\n",
    "\n",
    "Now we are left with the paramters from the first layer. You'll start to notice patterns forming here.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}} = \n",
    "\\color{blue}{\\frac{\\partial \\mathcal{L}}{\\partial \\hat Y}} \\cdot\n",
    "\\color{green}{\\frac{\\partial \\hat Y}{\\partial Z^{[2]}}} \\cdot\n",
    "\\color{magenta}{\\frac{\\partial Z^{[2]}}{\\partial A^{[1]}}} \\cdot\n",
    "\\color{olive}{\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}} \\cdot\n",
    "\\color{brown}{\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}}\n",
    "$$\n",
    "\n",
    "We already have the results for the first two terms. These will be propagated backward from layer 2 to layer 1. The third and final terms are new, but the second to last term is also something we've already seen (it depends on the ativation function).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{magenta}{\\frac{\\partial Z^{[2]}}{\\partial A^{[1]}}} \n",
    "&= \\frac{\\partial}{\\partial A^{[1]}} W^{[2]} A^{[1]} + b^{[2]}\\\\\n",
    "&= W^{[2]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{brown}{\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}}\n",
    "&= \\frac{\\partial}{\\partial W^{[1]}} W^{[1]} A^{[0]} + b^{[1]}\\\\\n",
    "&= A^{[0]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We again need to consider averaging the gradient for all $m$ examples and a use a specific activation funcion (again a sigmoid):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}} = \n",
    "\\frac{1}{m} W^{[2]T} (\\hat Y - Y) \\cdot (A^{[1]} \\cdot (1 - A^{[1]})) A^{[0]T}\\\\\n",
    "$$\n",
    "\n",
    "Looking at the shapes in the expession above, we have:\n",
    "\n",
    "$$\n",
    "(n1, n0) = ((n1, n2) (n2, m) \\times (n1, m)) (m, n0)\n",
    "$$\n",
    "\n",
    "which results in the same shape as $W^{[1]}$.\n",
    "\n",
    "One last equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}} = \n",
    "\\color{blue}{\\frac{\\partial \\mathcal{L}}{\\partial \\hat Y}} \\cdot\n",
    "\\color{green}{\\frac{\\partial \\hat Y}{\\partial Z^{[2]}}} \\cdot\n",
    "\\color{magenta}{\\frac{\\partial Z^{[2]}}{\\partial A^{[1]}}} \\cdot\n",
    "\\color{olive}{\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}} \\cdot\n",
    "\\color{brown}{\\frac{\\partial Z^{[1]}}{\\partial b^{[1]}}}\n",
    "$$\n",
    "\n",
    "The only new term is the last one:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{brown}{\\frac{\\partial Z^{[1]}}{\\partial b^{[1]}}}\n",
    "&= \\frac{\\partial}{\\partial b^{[1]}} W^{[1]} A^{[0]} + b^{[1]}\\\\\n",
    "&= 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "After rearranging:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}} = \n",
    "\\frac{1}{m} \\sum_{i=1}^m W^{[2]T} (\\hat Y - Y) \\cdot (A^{[1]} \\cdot (1 - A^{[1]}))\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of equations\n",
    "\n",
    "I've placed the sizes of each matrix in place.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "dZ^{[2]} \\color{gray}{(n_2, m)} &= \\hat Y - Y\\\\\n",
    "dW^{[2]} \\color{gray}{(n_2, n_1)} &= \\frac{1}{m} dZ^{[2]} A^{[1]T}\\\\\n",
    "db^{[2]} \\color{gray}{(n_2, 1)} &= \\frac{1}{m} \\sum_{i=1}^m dZ^{[2]}\\\\\n",
    "dZ^{[1]} \\color{gray}{(n_1, m)} &= W^{[2]T} dZ^{[2]} \\cdot A^{[1]} \\cdot (1 - A^{[1]})\\\\\n",
    "dW^{[1]} \\color{gray}{(n_1, n_0)} &= \\frac{1}{m} dZ^{[1]} X^T\\\\\n",
    "db^{[1]} \\color{gray}{(n_1, 1)} &= \\frac{1}{m} \\sum_{i=1}^m dZ^{[1]}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $X=A^{[0]}$ and $Y=A^{[2]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 (12, 100) : torch.Size([12, 100])\n",
      "db1 (12, 1) : torch.Size([12, 1])\n",
      "dW2 (4, 12) : torch.Size([4, 12])\n",
      "db2 (4, 1) : torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "def backward_propagation(\n",
    "    A0: Tensor, A1: Tensor, A2: Tensor, Y: Tensor, W2: Tensor\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Compute gradients of a 3-layer neural network's parameters.\n",
    "\n",
    "    Args:\n",
    "        A0 (Tensor): (n0, m) input matrix (aka X)\n",
    "        A1 (Tensor): (n1, m) output of layer 1 from forward propagation\n",
    "        A2 (Tensor): (n2, m) output of layer 2 from forward propagation\n",
    "        Y (Tensor): (n2, m) correct targets (aka labels)\n",
    "        W2 (Tensor): (n2, n1) weight matrix)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: gradients for weights and biases\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * (dZ2 @ A1.T)\n",
    "    db2 = (1 / m) * dZ2.sum(dim=1, keepdim=True)\n",
    "\n",
    "    dZ1 = (W2.T @ dZ2) * (A1 * (1 - A1))\n",
    "    dW1 = (1 / m) * (dZ1 @ A0.T)\n",
    "    db1 = (1 / m) * dZ1.sum(dim=1, keepdim=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "dW1, db1, dW2, db2 = backward_propagation(X, A1, A2, Y, W2)\n",
    "print(\"dW1\", (n1, n0), \":\", dW1.shape)\n",
    "print(\"db1\", (n1, 1), \":\", db1.shape)\n",
    "print(\"dW2\", (n2, n1), \":\", dW2.shape)\n",
    "print(\"db2\", (n2, 1), \":\", db2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating paramters\n",
    "\n",
    "The next step is to update the parameters. However, we don't want to overshoot the global minimum. So, we add an extra parameter, the *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (12, 100) : torch.Size([12, 100])\n",
      "b1 (12, 1) : torch.Size([12, 1])\n",
      "W2 (4, 12) : torch.Size([4, 12])\n",
      "b2 (4, 1) : torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "def update_parameters(\n",
    "    W1: Tensor,\n",
    "    b1: Tensor,\n",
    "    W2: Tensor,\n",
    "    b2: Tensor,\n",
    "    dW1: Tensor,\n",
    "    db1: Tensor,\n",
    "    dW2: Tensor,\n",
    "    db2: Tensor,\n",
    "    lr: float,\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Update parameters of a 3-layer neural network.\n",
    "\n",
    "    Args:\n",
    "        W1 (Tensor): (n1, n0) weight matrix\n",
    "        b1 (Tensor): (n1, 1) bias matrix)\n",
    "        W2 (Tensor): (n2, n1) weight matrix)\n",
    "        b2 (Tensor): (n2, 1) bias matrix\n",
    "        dW1 (Tensor): (n1, n0) gradient matrix\n",
    "        db1 (Tensor): (n1, 1) gradient matrix)\n",
    "        dW2 (Tensor): (n2, n1) gradient matrix)\n",
    "        db2 (Tensor): (n2, 1) gradient matrix\n",
    "        lr (float): learning rate\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: updated network parameters\n",
    "    \"\"\"\n",
    "    W1 = W1 - lr * dW1\n",
    "    b1 = b1 - lr * db1\n",
    "    W2 = W2 - lr * dW2\n",
    "    b2 = b2 - lr * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "print(\"W1\", (n1, n0), \":\", W1.shape)\n",
    "print(\"b1\", (n1, 1), \":\", b1.shape)\n",
    "print(\"W2\", (n2, n1), \":\", W2.shape)\n",
    "print(\"b2\", (n2, 1), \":\", b2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing cost\n",
    "\n",
    "To check our progress, we can compute the cost after each training epoch. We expect the cost to go down, which would indicate learning. We will have a **loss** value for each output for each example. **Cost** is the average loss value across all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost (4, 1) : torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "def compute_cost(A2: Tensor, Y: Tensor) -> Tensor:\n",
    "    \"\"\"Compute cost using binary cross entropy loss.\n",
    "\n",
    "    Args:\n",
    "        A2 (Tensor): (n2, m) matrix of neural network output values\n",
    "        Y (Tensor): (n2, m) correct targets (aka labels)\n",
    "\n",
    "    Returns:\n",
    "        float: computed cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    losses = -(Y * torch.log(A2) + (1 - Y) * torch.log(1 - A2))\n",
    "    cost = (1 / m) * losses.sum(dim=1, keepdims=True)\n",
    "    return cost\n",
    "\n",
    "\n",
    "cost = compute_cost(A2, Y)\n",
    "print(\"cost\", (n2, 1), \":\", cost.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/8: Cost=1.26\n",
      "  2/8: Cost=1.25\n",
      "  3/8: Cost=1.24\n",
      "  4/8: Cost=1.23\n",
      "  5/8: Cost=1.23\n",
      "  6/8: Cost=1.22\n",
      "  7/8: Cost=1.21\n",
      "  8/8: Cost=1.20\n"
     ]
    }
   ],
   "source": [
    "def learn(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    num_hidden: int,\n",
    "    param_scale: float,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"A function for performing batch gradient descent.\n",
    "\n",
    "    Args:\n",
    "        X (Tensor): (nx, m) matrix of input features\n",
    "        Y (Tensor): (n2, m) matrix of correct targets (aka labels)\n",
    "        num_hidden (int): number of neurons in layer 1\n",
    "        param_scale (float): scaling factor for initializing parameters\n",
    "        num_epochs (int): number of training passes through all data\n",
    "        learning_rate (float): learning rate\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: parameters of a 3-layer neural network\n",
    "    \"\"\"\n",
    "    n0 = X.shape[0]\n",
    "    n1 = num_hidden\n",
    "    n2 = Y.shape[0]\n",
    "\n",
    "    W1, b1, W2, b2 = initialize_parameters(n0, n1, n2, param_scale)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, A1, A2, Y, W2)\n",
    "\n",
    "        W1, b1, W2, b2 = update_parameters(\n",
    "            W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate\n",
    "        )\n",
    "\n",
    "        print(f\"{epoch + 1:>3}/{num_epochs}: Cost={cost.mean():.2f}\")\n",
    "        \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "num_epochs = 8\n",
    "learn(X, Y, n1, parameter_scale, num_epochs, learning_rate);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
